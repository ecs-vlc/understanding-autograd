{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse Mode Automatic Differentiation\n",
    "\n",
    "Dynamic Reverse mode AD can be implemented by declaring a class to represent a value and the child expressions that the value depends on. We've provided the implementation that was shown in the slides as a basis below, but it's missing some parts that will make it useful.\n",
    "\n",
    "__Tasks:__\n",
    "\n",
    "- Addition (`__add__`) is incomplete - can you finish it? \n",
    "- Can you also implement division (`__div__`), subtraction (`__sub__`) and power (`__pow__`)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Var:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.children = []\n",
    "        self.grad_value = None\n",
    "\n",
    "    def grad(self):\n",
    "        if self.grad_value is None:\n",
    "            self.grad_value = sum(weight * var.grad()\n",
    "                                  for weight, var in self.children)\n",
    "        return self.grad_value\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        z = Var(self.value * other.value)\n",
    "        self.children.append((other.value, z))\n",
    "        other.children.append((self.value, z))\n",
    "        return z\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return None # fix me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like when we were looking at Forward Mode AD, we also need to implement some core math functions. Here's the sine function for a `Var`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin(x):\n",
    "    z = Var(math.sin(x.value))\n",
    "    x.children.append((math.cos(x.value), z))\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task:__ can you implement the _cosine_ (`cos`), _tangent_ (`tan`), and _exponential_ (`exp`) functions in the code block below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement additional math functions on dual numbers\n",
    "\n",
    "def cos(x):\n",
    "    return None #TODO: fix me\n",
    "\n",
    "def tan(x):\n",
    "    return None #TODO: fix me\n",
    "\n",
    "def exp(x):\n",
    "    return None #TODO: fix me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now in a position to try our implementation.\n",
    "\n",
    "__Tasks:__ \n",
    "\n",
    "- Try running the following code to compute the value of the function $z=x\\cdot y+sin(x)$ given $x=0.5$ and $y=4.2$, together with the derivative $\\partial z/\\partial x$ at that point. \n",
    "- Verify that the result is correct by hand-differentiating the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Var(0.5)\n",
    "y = Var(4.2)\n",
    "z = x * y + sin(x)\n",
    "print(z)\n",
    "\n",
    "z.grad_value = 1.0 #Note that we have to 'seed' the gradient of z to 1 (e.g. ∂z/∂z=1) before computing grads\n",
    "print(x.grad())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task:__ Now use the code block below to compute and print the derivative $\\partial z/\\partial y$ of the above expression (at the same point $x=0.5, y=4.2$ as above) and verify by hand that the result is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at doing something wacky: differentiate an algorithm. For this example, we'll use an algorithm that is in a sense static (in this particular case the upper limit of the for loop is predetermined). However, it is not difficult to see that AD is much more general, and could even be applied to stochastic algorithms (say if we replaced the upper limit of the loop below with `Math.floor(Math.random() * 10)` for example).\n",
    "\n",
    "__Task:__ Consider the following algorithm and manually compute the value of $z$ and the gradient $\\partial z/\\partial x$ at the end of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Var(0.5)\n",
    "z = Var(1)\n",
    "for i in range(0,2):\n",
    "    z = (z + Var(i)) * x * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task__: Now use the code block below to print out the gradient computed by our reverse AD. Does it match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task:__ Finally, use the code block below to experiment and test the other math functions and methods you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
